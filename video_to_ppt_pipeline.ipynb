{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e1PCncG-U_R"
      },
      "outputs": [],
      "source": [
        "GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW6pRkBM_fqK"
      },
      "outputs": [],
      "source": [
        "!pip install python-pptx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSqzFt5m_wG3"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update && sudo apt-get install libreoffice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cK4eoBDo7upH",
        "outputId": "b175790b-49fa-4016-9c3f-c1c1695b6270"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0XhKuV3y8z5k",
        "outputId": "4bbad1a8-92f2-4d6f-e5e3-c023acaf4fbd"
      },
      "outputs": [],
      "source": [
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "znC29vfA8z8p",
        "outputId": "dacd3172-122b-494d-96f5-5f1191cdc0dc"
      },
      "outputs": [],
      "source": [
        "!pip install yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oMscsrRy8z_T",
        "outputId": "c8e822a0-1517-46fb-9728-699ab1f7a5d0"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfI8n2qC80CD",
        "outputId": "b9a34dba-256b-463c-a653-2db44ab3023d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/live/D7BzTxVVMuw?t=19684s\n",
            "[youtube] D7BzTxVVMuw: Downloading webpage\n",
            "[youtube] D7BzTxVVMuw: Downloading tv client config\n",
            "[youtube] D7BzTxVVMuw: Downloading player 5ae7d525\n",
            "[youtube] D7BzTxVVMuw: Downloading tv player API JSON\n",
            "[youtube] D7BzTxVVMuw: Downloading ios player API JSON\n",
            "[youtube] D7BzTxVVMuw: Downloading m3u8 information\n",
            "[info] D7BzTxVVMuw: Downloading 1 format(s): 243+251\n",
            "[download] Destination: downloaded_video.f243.webm\n",
            "[download] 100% of  416.05MiB in 00:00:20 at 20.44MiB/s  Download finished, saved as downloaded_video.f243.webm\n",
            "\n",
            "[download] Destination: downloaded_video.f251.webm\n",
            "[download] 100% of  397.36MiB in 00:00:09 at 43.77MiB/s  Download finished, saved as downloaded_video.f251.webm\n",
            "\n",
            "[Merger] Merging formats into \"downloaded_video.webm\"\n",
            "Deleting original file downloaded_video.f251.webm (pass -k to keep)\n",
            "Deleting original file downloaded_video.f243.webm (pass -k to keep)\n",
            "downloaded_video.f251.webm\n"
          ]
        }
      ],
      "source": [
        "import yt_dlp\n",
        "import subprocess\n",
        "import os\n",
        "import glob\n",
        "import whisper\n",
        "\n",
        "audio_segments_folder = \"audio_segments\"\n",
        "transcription_output = \"transcription.txt\"\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(audio_segments_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "# Step 1: Download the video if a URL is provided\n",
        "video_url = \"https://www.youtube.com/live/D7BzTxVVMuw?t=19684s\"  # Replace with your video URL\n",
        "\n",
        "# or you might want to analyse the uploaded video file.\n",
        "\n",
        "UPLOADED_FILE = None # Replace with the path of your uploaded file\n",
        "\n",
        "# Global variable to capture the downloaded file name\n",
        "downloaded_file = None\n",
        "\n",
        "# Define a progress hook to capture the filename when download finishes\n",
        "def progress_hook(d):\n",
        "    global downloaded_file\n",
        "    if d.get('status') == 'finished':\n",
        "        downloaded_file = d.get('filename')\n",
        "        print(f\"Download finished, saved as {downloaded_file}\")\n",
        "\n",
        "# Step 1: Download the video in lower quality (limit resolution to 480p) and capture filename\n",
        "ydl_opts = {\n",
        "    'format': 'bestvideo[height<=480]+bestaudio/best[height<=480]',\n",
        "    'outtmpl': 'downloaded_video.%(ext)s',\n",
        "    'progress_hooks': [progress_hook]\n",
        "}\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([video_url])\n",
        "\n",
        "if downloaded_file is None:\n",
        "    raise Exception(\"Download failed: no file was captured.\")\n",
        "\n",
        "print(downloaded_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XAXyd5R4A45O",
        "outputId": "bec0e131-8e61-4d2e-ab11-cb8ae3ca4fc8"
      },
      "outputs": [],
      "source": [
        "# extract a short video to check if fine\n",
        "!ffmpeg -ss 00:10:00 -to 00:13:00 -i downloaded_video.webm -c copy output.webm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlIG51lxATdU"
      },
      "outputs": [],
      "source": [
        "if not downloaded_file and UPLOADED_FILE:\n",
        "  downloaded_file = UPLOADED_FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl92A9Eo80Ej"
      },
      "outputs": [],
      "source": [
        "# Step 1: Extract audio from the downloaded video (audio-only extraction)\n",
        "original_audio = \"original_audio.mp3\"\n",
        "ffmpeg_extract_audio_cmd = [\n",
        "    'ffmpeg', '-i', downloaded_file,\n",
        "    '-q:a', '0', '-map', 'a',\n",
        "    original_audio\n",
        "]\n",
        "subprocess.run(ffmpeg_extract_audio_cmd)\n",
        "\n",
        "## CompletedProcess(args=['ffmpeg', '-i', '/content/downloaded_video.mkv', '-q:a', '0', '-map', 'a', 'original_audio.mp3'], returncode=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv0vpsJj80HM",
        "outputId": "9cd50d47-ce05-4dc0-b1dc-9a5fd2d8f966"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['ffmpeg', '-i', 'original_audio.mp3', '-filter:a', 'atempo=1.2', 'spedup_audio.mp3'], returncode=0)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 2: Speed up the extracted audio by 1.2x using the atempo filter\n",
        "spedup_audio = \"spedup_audio.mp3\"\n",
        "ffmpeg_speedup_audio_cmd = [\n",
        "    'ffmpeg', '-i', original_audio,\n",
        "    '-filter:a', \"atempo=1.2\",\n",
        "    spedup_audio\n",
        "]\n",
        "subprocess.run(ffmpeg_speedup_audio_cmd)\n",
        "\n",
        "## CompletedProcess(args=['ffmpeg', '-i', 'original_audio.mp3', '-filter:a', 'atempo=1.2', 'spedup_audio.mp3'], returncode=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBzHN1Nz80J0"
      },
      "outputs": [],
      "source": [
        "# Step 3: Split the sped-up audio into 1-minute segments, saved in the dedicated folder\n",
        "split_audio_cmd = [\n",
        "    'ffmpeg', '-i', spedup_audio,\n",
        "    '-f', 'segment', '-segment_time', '60', '-c', 'copy',\n",
        "    os.path.join(audio_segments_folder, 'audio_segment%03d.mp3')\n",
        "]\n",
        "subprocess.run(split_audio_cmd)\n",
        "\n",
        "## CompletedProcess(args=['ffmpeg', '-i', 'spedup_audio.mp3', '-f', 'segment', '-segment_time', '60', '-c', 'copy', 'audio_segments/audio_segment%03d.mp3'], returncode=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR200L-J80Ma"
      },
      "outputs": [],
      "source": [
        "# Step 4: Load the Whisper model and transcribe each audio segment,\n",
        "# writing the transcriptions (with timestamps) into a text file.\n",
        "model = whisper.load_model(\"base\")  # Options: \"base\", \"small\", \"medium\", \"large\"\n",
        "\n",
        "with open(transcription_output, 'w') as outfile:\n",
        "    # Get a sorted list of audio segment files from the folder\n",
        "    audio_segments = sorted(glob.glob(os.path.join(audio_segments_folder, \"audio_segment*.mp3\")))\n",
        "\n",
        "    for seg in audio_segments:\n",
        "        outfile.write(f\"Transcription for {seg}:\\n\")\n",
        "        print(f\"Transcribing {seg}...\")\n",
        "        result = model.transcribe(seg)\n",
        "\n",
        "        # Write the overall transcription text\n",
        "        outfile.write(\"Full Transcription:\\n\")\n",
        "        outfile.write(result[\"text\"] + \"\\n\")\n",
        "\n",
        "        # Write each segment with timestamps\n",
        "        outfile.write(\"Timestamps and Text Segments:\\n\")\n",
        "        for s in result.get(\"segments\", []):\n",
        "            start = s.get(\"start\")\n",
        "            end = s.get(\"end\")\n",
        "            text = s.get(\"text\")\n",
        "            outfile.write(f\"{start:6.2f}s to {end:6.2f}s: {text}\\n\")\n",
        "        outfile.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Transcriptions have been written to {transcription_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqc46c6p80PH"
      },
      "outputs": [],
      "source": [
        "all_text = \"\"\n",
        "capture = False\n",
        "with open(transcription_output, 'r') as infile:\n",
        "    for line in infile:\n",
        "        stripped = line.strip()\n",
        "        # Start capturing when we hit the \"Full Transcription:\" header.\n",
        "        if stripped == \"Full Transcription:\":\n",
        "            capture = True\n",
        "            continue\n",
        "        # Stop capturing when the timestamps section starts.\n",
        "        if stripped == \"Timestamps and Text Segments:\":\n",
        "            capture = False\n",
        "        # Append captured lines.\n",
        "        if capture and stripped:\n",
        "            all_text += stripped + \" \"\n",
        "# print(all_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pbeUHJgQ80R0"
      },
      "outputs": [],
      "source": [
        "# check num of words\n",
        "len(all_text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lGEtwfM80Un"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMk6Mqnd80XG"
      },
      "outputs": [],
      "source": [
        "SEGMENT_EXTRACTION_PROMPT= \"\"\"Your task is to analyze the provided text and divide it into coherent segments.\n",
        "This text is transcribed from a video and may contain random sounds or incoherent phrases which could not be understood properly.\n",
        "The number of segments to be made is left to your judgement but it should not exceed 20.\n",
        "For each segment, please generate a JSON object containing the following keys:\n",
        "\n",
        "1. \"segment_number\": Provide the serial number of the segment.\n",
        "2. \"short_title\": Provide a short title for this segment . max 5 to 7 words\n",
        "3. \"summary\": Provide a concise summary of the segment's key ideas. You must explain the tricks and ideas discussed in a concise manner such that a person reading the summary can understand the key elements discussed.\n",
        "4. \"justification\": Explain why you believe this segment is valuable and worthy of attention.\n",
        "5. \"counter_intuitive\": Identify any points that are counter-intuitive or surprising, and that are worth remembering.\n",
        "6. \"topics\": List topics or tags related to this segment that could be explored further to enhance understanding.\n",
        "7. \"questions\": Generate a set of questions that can test the user's understanding of the segment.\n",
        "\n",
        "The final output should be a valid JSON object containing an array of segments. The JSON must strictly follow the format below:\n",
        "\n",
        "{\n",
        "  \"segments\": [\n",
        "    {\n",
        "      \"segment_number\": \"<int num>\",\n",
        "      \"short_title\": \"title of the segment\",\n",
        "      \"summary\": \"A concise summary of the segment and the important ideas with a short explanation.\",\n",
        "      \"justification\": \"The reason this segment is significant.\",\n",
        "      \"counter_intuitive\": \"Key counter-intuitive or surprising points that are important to remember.\",\n",
        "      \"topics\": [\"topic1\", \"topic2\", \"...\"],\n",
        "      \"questions\": [\"Question 1?\", \"Question 2?\", \"...\"]\n",
        "    }\n",
        "    // ... additional segments as needed\n",
        "  ]\n",
        "}\n",
        "\n",
        "Ensure that the output is a valid JSON with no additional keys or formatting errors.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxow7XL780Zw"
      },
      "outputs": [],
      "source": [
        "messages_for_llm = []\n",
        "messages_for_llm.append({\"role\": \"system\", \"content\": SEGMENT_EXTRACTION_PROMPT})\n",
        "messages_for_llm.append({\"role\": \"user\", \"content\": f\"The transcribed text is as follows: {all_text}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCNAEV0R80cH"
      },
      "outputs": [],
      "source": [
        "llm_kwargs = {\"model\": \"gemini-2.0-flash\",\n",
        "                            \"temperature\": 0.2,\n",
        "                            \"response_format\": {\"type\": \"json_object\"}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUNQiFWA80fF"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "            messages=messages_for_llm,\n",
        "            **llm_kwargs\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5-oeNFZ80hh"
      },
      "outputs": [],
      "source": [
        "result_json = completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52zHQQm-80j5"
      },
      "outputs": [],
      "source": [
        "def trim_incomplete_json(json_str):\n",
        "    stack = 0\n",
        "    last_index = -1\n",
        "    for i, char in enumerate(json_str):\n",
        "        if char == '{':\n",
        "            stack += 1\n",
        "        elif char == '}':\n",
        "            stack -= 1\n",
        "            # When the stack is balanced, record this index\n",
        "            if stack == 0:\n",
        "                last_index = i\n",
        "    # If we found a valid end, slice the string up to that point.\n",
        "    if last_index != -1:\n",
        "        return json_str[:last_index+1]\n",
        "    return json_str  # return original if no valid end found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIwxuTQK80mW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('transcription_result.json', 'w') as fp:\n",
        "    json.dump(json.loads(result_json), fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZMCitML80oT"
      },
      "outputs": [],
      "source": [
        "QUESTION_ANSWER_PROMPT = \"\"\"You are provided with a list of questions related to the transcribed text from a video.\n",
        "For each question, please provide a concise answer that captures the essence of the concept strictly from the content discussed in the text.\n",
        "DO NOT make up answers on your own.\n",
        "Your output must be valid JSON in the following format:\n",
        "\n",
        "{\n",
        "  \"answers\": [\n",
        "    {\n",
        "      \"question\": \"Question 1?\",\n",
        "      \"answer\": \"Concise answer for question 1.\"\n",
        "    },\n",
        "    {\n",
        "      \"question\": \"Question 2?\",\n",
        "      \"answer\": \"Concise answer for question 2.\"\n",
        "    }\n",
        "    // ... more question-answer pairs as needed\n",
        "  ]\n",
        "}\n",
        "\n",
        "Do not include any additional keys or commentary.\n",
        "Make sure that each answer is short, clear, and directly addresses the question in an understandable way.\n",
        "Please provide your answers in the specified JSON format.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4JJJtU6_cxr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "# Placeholder function: Replace with your actual LLM API call\n",
        "def call_llm_api(messages_for_llm: list[dict]) -> dict:\n",
        "    \"\"\"\n",
        "    This function should call your LLM with the given prompt and return its response as a string.\n",
        "    For demonstration, we simulate a response.\n",
        "    \"\"\"\n",
        "    parsed_json = None\n",
        "    result_json = None\n",
        "    completion = client.chat.completions.create(\n",
        "                messages=messages_for_llm,\n",
        "                **llm_kwargs\n",
        "            )\n",
        "    result_json = completion.choices[0].message.content\n",
        "    fixed_result = trim_incomplete_json(result_json)\n",
        "    try:\n",
        "        parsed_json = json.loads(fixed_result)\n",
        "        print(\"Successfully parsed JSON!\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Still not valid JSON:\", e)\n",
        "    if parsed_json:\n",
        "      return parsed_json\n",
        "    else:\n",
        "      return result_json\n",
        "\n",
        "# Define the prompt template for extracting answers\n",
        "def build_answer_prompt(system_prompt, transcribed_text, questions):\n",
        "    user_input = f\"The transcribed text is as follows: {transcribed_text}\\n\\n\"\n",
        "    user_input += f\"The Questions to be answered are as follows: {questions}\"\n",
        "    messages_for_llm = []\n",
        "    messages_for_llm.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    messages_for_llm.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    return messages_for_llm\n",
        "\n",
        "# List to hold final question-answer pairs\n",
        "final_answers = []\n",
        "\n",
        "parsed_json = json.loads(result_json)\n",
        "\n",
        "# Iterate over each segment in the parsed JSON\n",
        "for segment in parsed_json.get(\"segments\", []):\n",
        "    segment_num = segment.get(\"segment_number\")\n",
        "    questions = segment.get(\"questions\", [])\n",
        "    if not questions:\n",
        "        continue\n",
        "\n",
        "    # Build the prompt for the current set of questions.\n",
        "    prompt = build_answer_prompt(QUESTION_ANSWER_PROMPT , all_text, questions)\n",
        "\n",
        "    # Call the LLM API to get the answers for this segment\n",
        "    llm_response = call_llm_api(prompt)\n",
        "    if llm_response:\n",
        "\n",
        "        try:\n",
        "\n",
        "            # Iterate over the answers and add them to the final list\n",
        "            for qa in llm_response.get(\"answers\", []):\n",
        "                final_answers.append({\n",
        "                    \"segment\": segment_num,\n",
        "                    \"question\": qa.get(\"question\", \"\"),\n",
        "                    \"answer\": qa.get(\"answer\", \"\")\n",
        "                })\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(\"Error parsing LLM response:\", e)\n",
        "    time.sleep(3)\n",
        "\n",
        "# Dump the final answers into a JSON file\n",
        "output_file = \"final_answers.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(final_answers, f, indent=2)\n",
        "\n",
        "print(f\"All question-answer pairs have been saved in '{output_file}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG-FitjL_c0y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import textwrap\n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt\n",
        "\n",
        "# Load your JSON data (adjust file names as needed)\n",
        "with open(\"/content/transcription_result.json\", \"r\") as f:\n",
        "    segment_data = json.load(f)[\"segments\"]\n",
        "\n",
        "with open(\"final_answers.json\", \"r\") as f:\n",
        "    answers_data = json.load(f)\n",
        "\n",
        "# Group Q&A pairs by segment_number\n",
        "answers_by_segment = {}\n",
        "for qa in answers_data:\n",
        "    seg_num = qa[\"segment\"]\n",
        "    answers_by_segment.setdefault(seg_num, []).append(qa)\n",
        "\n",
        "# Create a new PowerPoint presentation\n",
        "prs = Presentation()\n",
        "\n",
        "# Helper function to add wrapped text to a textbox\n",
        "def add_wrapped_paragraph(text_frame, text, font_size=Pt(20), bold=False, bullet=False):\n",
        "    # Enable word wrapping\n",
        "    text_frame.word_wrap = True\n",
        "    p = text_frame.add_paragraph()\n",
        "    p.text = text\n",
        "    p.font.size = font_size\n",
        "    p.font.bold = bold\n",
        "    if bullet:\n",
        "        p.level = 0\n",
        "    return p\n",
        "\n",
        "for seg in segment_data:\n",
        "    seg_num = seg.get(\"segment_number\")\n",
        "\n",
        "    # --- Slide 1: Title Slide ---\n",
        "    slide1 = prs.slides.add_slide(prs.slide_layouts[5])  # Blank layout\n",
        "    title_box = slide1.shapes.add_textbox(Inches(0.5), Inches(0.5), Inches(9), Inches(1.5))\n",
        "    tf_title = title_box.text_frame\n",
        "    tf_title.word_wrap = True\n",
        "    short_title = seg.get(\"short_title\", f\"Segment {seg_num}\")\n",
        "    add_wrapped_paragraph(tf_title, short_title, font_size=Pt(36), bold=True)\n",
        "\n",
        "    # --- Slide 2: Summary, Counter-Intuitive Points & Topics ---\n",
        "    slide2 = prs.slides.add_slide(prs.slide_layouts[5])\n",
        "    summary_box = slide2.shapes.add_textbox(Inches(0.5), Inches(0.5), Inches(9), Inches(5))\n",
        "    tf_summary = summary_box.text_frame\n",
        "    tf_summary.word_wrap = True\n",
        "    add_wrapped_paragraph(tf_summary, \"Summary & Key Points\", font_size=Pt(32), bold=True)\n",
        "\n",
        "    summary_text = f\"Summary: {seg.get('summary', '')}\\n\\n\"\n",
        "    summary_text += f\"Counter-Intuitive Points: {seg.get('counter_intuitive', '')}\\n\\n\"\n",
        "    topics = seg.get('topics', [])\n",
        "    summary_text += f\"Topics to Explore: {', '.join(topics)}\"\n",
        "    add_wrapped_paragraph(tf_summary, summary_text, font_size=Pt(20))\n",
        "\n",
        "    # --- Slide 3: Q&A Slide ---\n",
        "    slide3 = prs.slides.add_slide(prs.slide_layouts[5])\n",
        "    qa_box = slide3.shapes.add_textbox(Inches(0.5), Inches(0.5), Inches(9), Inches(5))\n",
        "    tf_qa = qa_box.text_frame\n",
        "    tf_qa.word_wrap = True\n",
        "    add_wrapped_paragraph(tf_qa, \"Q&A\", font_size=Pt(32), bold=True)\n",
        "\n",
        "    qa_list = answers_by_segment.get(seg_num, [])\n",
        "    for qa in qa_list:\n",
        "        question = qa.get(\"question\", \"\")\n",
        "        answer = qa.get(\"answer\", \"\")\n",
        "        qa_text = f\"Q: {question}\\nA: {answer}\\n\"\n",
        "        add_wrapped_paragraph(tf_qa, qa_text, font_size=Pt(20), bullet=True)\n",
        "\n",
        "\n",
        "# Save the presentation to a PPTX file\n",
        "pptx_file = \"presentation.pptx\"\n",
        "prs.save(pptx_file)\n",
        "print(f\"Presentation saved as {pptx_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u331f0KT_c3n"
      },
      "outputs": [],
      "source": [
        "pdf_file = pptx_file.replace(\".pptx\", \".pdf\")\n",
        "try:\n",
        "    # Run LibreOffice headless conversion\n",
        "    conversion = subprocess.run(\n",
        "        [\"libreoffice\", \"--headless\", \"--convert-to\", \"pdf\", pptx_file],\n",
        "        check=True,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE\n",
        "    )\n",
        "    print(\"LibreOffice output:\", conversion.stdout.decode(\"utf-8\"))\n",
        "    if os.path.exists(pdf_file):\n",
        "        print(f\"Presentation successfully converted to PDF: {pdf_file}\")\n",
        "    else:\n",
        "        print(\"PDF conversion did not produce output. Please ensure LibreOffice is installed and in your PATH.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"PDF conversion failed:\", e.stderr.decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWYV-gNf_c6o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
